This work directly builds on Madureira and Schlangen, 2024, hence the setup, as well as this ReadMe is derived from their work.
# Instruction Clarification Requests in the CoDraw Dataset

This is the code repository accompanying the following Thesis:

- Generating Instruction Clarification Request in the CoDraw Dataset

We implement hierarchical Transformer-based models to try to immitate human iCRs generation.


## Description

The directories are:

- ```lightning_logs/```: contains the trained model checkpoints at their best validation epoch.
- ```comet-logs/```: contains the trained model checkpoints at their best validation epoch, when logged by comet.
- ```codrawmodels/```: contains one script from the CoDraw authors with minor adaptations to make it work in our setting.
- ```../data/```: is where all downloaded data should live.
- ```env/```: contains the files to reconstruct the conda environment (see below).
- ```icr/```: code of our implementation.
- ```notebooks/```: juypter notebooks used for evaluation.
- ```outputs/```: the generated outputs of the experiments.


## Dependencies

The directory ```env/``` contains the files that can be used to recreate the conda environment.

```bash
conda env create -f conda-environment.yml 
```

is currently the most consistent way to set up a conda environment, which enables the execution of the code. Note that not the most recent versions of PyTorch due to the cuda version available in the development environment. 

Another option is running

```bash
sh scripts/setup.sh
```

which execute the same installations one by one as we did. In case is does not work, this directory also contain the .yml files and a spec file auto-generated by comet.ml.


## Data

Four data assets are necessary to run these scripts. They should live at ```../data```,
or the paths to each one can be passed as command-line arguments:

- CoDraw: data code, download from [link](https://github.com/facebookresearch/CoDraw). Should replace ```codrawmodels/```, except for the file we had to adjust (```codrawmodels/codrawmodels/codraw_data.py```). This is necessary for computing scene similatiry scores.
- CoDraw: data file, download from [link](https://drive.google.com/file/d/0B-u9nH58139bTy1XRFdqaVEzUGs/view?usp=sharing).
- CoDraw-iCR (v2) annotation: available at OSF: [https://osf.io/gcjhz/](https://osf.io/gcjhz/). It is possible to download it manually or clone it via the [osfclient](https://github.com/osfclient/osfclient).
- Step-by-step scenes: follow the stes at [this repository](https://github.com/briemadu/codraw-icr-v1/tree/main/data/IncrementalCoDrawImages). The images must be saved as numpy arrays using h5py objects, as done [here](https://github.com/briemadu/codraw-icr-v1/blob/2eefa9d1382dda5dfd30933a5a25398538d7c9a2/data/preprocessed/img_embeddings.py#L182).
- AbstractScenes download at this [link](https://www.microsoft.com/en-ca/download/details.aspx?id=52035)

## Replicating the results

The results can be replicated by regenerating the pretrained embeddings and then calling the bash script that runs all experiments. ```search.py``` was used for hyperparameter search.

```bash
mkdir outputs
mkdir ../data/text_embeddings/
python3 scripts/get_text_embeddings.py -model bert-base-uncased
python3 scripts/get_text_embeddings.py -model roberta-base
python3 scripts/get_text_embeddings.py -model distilbert-base-uncased
python3 scripts/get_bounding_boxes.py
python3 main.py
```

## General usage

```main.py``` is used to train a new model. It performs only training and saving the model, however, the performance of the model is saved in .txt files in the outputs directory.
To adjust the model configuration and hyperparameter, please adjust the ```icr/config.py``` file to your preferences.

```bash
python3 reload.py
```
can be used to recreate outputs for the validation data of saved model. Especially, the impact of different inputs to sampling and temperature can be explored here. 

## Credits

We thank the developers of all the open libraries we use: comet.ml, csv, h5py, matplotlib, numpy, pandas, PIL, positional_encodings, python, pytorch, lightning, scikit-learn, scipy, seaborn, torchmetrics, torchvision, transformers, tqdm.

This work is based on the CoDraw dataset (Kim et al, 2019), AbstractScenes (Microsoft) and CoDraw-iCR (v2) (Madureira and Schlangen, 2023b). As this thesis is a direct successor of Madureira's and Schlangen's latest work, we want to thank them for providing well documented and structured code, without which this work would not have been possible.

## License

Our source code is licensed under the MIT License.

